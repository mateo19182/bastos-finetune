Dataset size: 200
==((====))==  Unsloth: Fast Llama patching release 2024.7
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.69 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters
are not enabled or a bias term (like in Qwen) is used.
Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters
are not enabled or a bias term (like in Qwen) is used.
Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters
are not enabled or a bias term (like in Qwen) is used.
Unsloth 2024.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
Map (num_proc=2): 100%|██████████| 200/200 [00:00<00:00, 374.57 examples/s]
max_steps is given, it will override any value given in num_train_epochs
Starting training...
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 200 | Num Epochs = 4
O^O/ \_/ \    Batch size per device = 2 | Gradient Accumulation steps = 4
\        /    Total batch size = 8 | Total steps = 80
 "-____-"     Number of trainable parameters = 41,943,040
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.

  1%|▏         | 1/80 [00:07<10:27,  7.95s/it]

  2%|▎         | 2/80 [00:13<08:15,  6.35s/it]

  4%|▍         | 3/80 [00:40<20:15, 15.78s/it]

  5%|▌         | 4/80 [00:44<14:30, 11.45s/it]
{'loss': 2.7391, 'grad_norm': 0.6037139892578125, 'learning_rate': 0.0004, 'epoch': 0.16}


  8%|▊         | 6/80 [01:17<16:13, 13.15s/it]
{'loss': 2.6722, 'grad_norm': 0.47797271609306335, 'learning_rate': 0.0004933333333333334, 'epoch': 0.24}


 10%|█         | 8/80 [01:46<17:55, 14.94s/it]

 11%|█▏        | 9/80 [01:50<13:38, 11.52s/it]

 12%|█▎        | 10/80 [02:14<17:56, 15.38s/it]

 14%|█▍        | 11/80 [02:19<13:45, 11.96s/it]

 15%|█▌        | 12/80 [02:26<12:02, 10.63s/it]

 16%|█▋        | 13/80 [02:52<17:09, 15.36s/it]

 18%|█▊        | 14/80 [03:00<14:27, 13.14s/it]

 19%|█▉        | 15/80 [03:25<17:51, 16.48s/it]

 20%|██        | 16/80 [03:30<13:58, 13.11s/it]

 21%|██▏       | 17/80 [03:35<11:18, 10.77s/it]

 22%|██▎       | 18/80 [04:00<15:22, 14.88s/it]
{'loss': 2.1876, 'grad_norm': 0.4054321050643921, 'learning_rate': 0.0004133333333333333, 'epoch': 0.72}


 25%|██▌       | 20/80 [04:27<15:01, 15.03s/it]

 26%|██▋       | 21/80 [04:31<11:35, 11.79s/it]

 28%|██▊       | 22/80 [04:35<09:09,  9.48s/it]

 29%|██▉       | 23/80 [05:04<14:28, 15.23s/it]

 30%|███       | 24/80 [05:09<11:26, 12.25s/it]

 31%|███▏      | 25/80 [05:33<14:26, 15.76s/it]

 32%|███▎      | 26/80 [05:39<11:41, 12.98s/it]
{'loss': 2.0956, 'grad_norm': 0.4295581579208374, 'learning_rate': 0.00035999999999999997, 'epoch': 1.04}


 35%|███▌      | 28/80 [06:08<12:36, 14.55s/it]

 36%|███▋      | 29/80 [06:12<09:46, 11.51s/it]

 38%|███▊      | 30/80 [06:41<13:51, 16.62s/it]

 39%|███▉      | 31/80 [06:44<10:21, 12.69s/it]

 40%|████      | 32/80 [06:52<08:53, 11.11s/it]

 41%|████▏     | 33/80 [07:18<12:20, 15.75s/it]

 42%|████▎     | 34/80 [07:25<09:53, 12.90s/it]

 44%|████▍     | 35/80 [07:50<12:23, 16.52s/it]

 45%|████▌     | 36/80 [07:55<09:38, 13.16s/it]

 46%|████▋     | 37/80 [08:00<07:44, 10.80s/it]

 48%|████▊     | 38/80 [08:24<10:20, 14.78s/it]

 49%|████▉     | 39/80 [08:32<08:34, 12.54s/it]

 50%|█████     | 40/80 [08:59<11:14, 16.86s/it]

 51%|█████▏    | 41/80 [09:03<08:32, 13.13s/it]

 52%|█████▎    | 42/80 [09:08<06:46, 10.69s/it]

 54%|█████▍    | 43/80 [09:32<09:05, 14.75s/it]

 55%|█████▌    | 44/80 [09:40<07:36, 12.69s/it]

 56%|█████▋    | 45/80 [10:04<09:19, 15.99s/it]

 57%|█████▊    | 46/80 [10:08<07:07, 12.57s/it]

 59%|█████▉    | 47/80 [10:14<05:43, 10.40s/it]

 60%|██████    | 48/80 [10:39<08:00, 15.01s/it]

 61%|██████▏   | 49/80 [10:44<06:04, 11.75s/it]

 62%|██████▎   | 50/80 [11:08<07:45, 15.51s/it]

 64%|██████▍   | 51/80 [11:13<06:02, 12.49s/it]

 65%|██████▌   | 52/80 [11:19<04:48, 10.32s/it]

 66%|██████▋   | 53/80 [11:43<06:29, 14.43s/it]

 68%|██████▊   | 54/80 [11:49<05:13, 12.04s/it]

 69%|██████▉   | 55/80 [12:14<06:41, 16.04s/it]

 70%|███████   | 56/80 [12:18<04:57, 12.39s/it]

 71%|███████▏  | 57/80 [12:24<03:59, 10.40s/it]

 72%|███████▎  | 58/80 [12:51<05:34, 15.22s/it]

 74%|███████▍  | 59/80 [12:58<04:27, 12.75s/it]

 75%|███████▌  | 60/80 [13:24<05:35, 16.77s/it]

 76%|███████▋  | 61/80 [13:28<04:08, 13.06s/it]

 78%|███████▊  | 62/80 [13:33<03:10, 10.60s/it]

 79%|███████▉  | 63/80 [13:58<04:15, 15.01s/it]

 80%|████████  | 64/80 [14:05<03:21, 12.57s/it]

 81%|████████▏ | 65/80 [14:32<04:13, 16.90s/it]

 82%|████████▎ | 66/80 [14:36<03:01, 12.97s/it]

 84%|████████▍ | 67/80 [14:41<02:18, 10.65s/it]
